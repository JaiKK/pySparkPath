<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pyspark on pySpark Learning Path</title>
    <link>https://jaikk.github.io/pySparkPath/pyspark/</link>
    <description>Recent content in Pyspark on pySpark Learning Path</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Jan 2021 21:34:39 +1100</lastBuildDate><atom:link href="https://jaikk.github.io/pySparkPath/pyspark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Installation</title>
      <link>https://jaikk.github.io/pySparkPath/pyspark/installation/</link>
      <pubDate>Sun, 31 Jan 2021 23:33:26 +1100</pubDate>
      
      <guid>https://jaikk.github.io/pySparkPath/pyspark/installation/</guid>
      <description>Create Dockerfile This installation method will cover installation of Spark in local system (Linux) and docker container. I have attached Dockerfile with this tutorial and explained each line and its purpose.
I have used Ubuntu for this tutorial but you can choose any linux distribution. Windows I&amp;rsquo;ll cover is separate tutorial.
In this tutorial I have downloaded the spark installer (compressed file) for convenience. But I&amp;rsquo;ll also add commands to download it directly from server while installation.</description>
    </item>
    
    <item>
      <title>Running</title>
      <link>https://jaikk.github.io/pySparkPath/pyspark/running/</link>
      <pubDate>Sun, 14 Feb 2021 08:44:28 +1100</pubDate>
      
      <guid>https://jaikk.github.io/pySparkPath/pyspark/running/</guid>
      <description>After installation its time to test our Spark installation. we&amp;rsquo;ll test in below order:
 Notebook SHELL Cluster  Use below command to run Docker container C:\&amp;gt; docker run -dit -p4040:4040 -p8888:8888 -p18080:18080 -p8080:8080 -p8081:8081 -p7077:7077 --name pySparkCluster --hostname pySparkCluster jai/pyspark bash Pay attention that we have mapped all port to localhost and defined container hostname to &amp;lsquo;pySparkCluster&amp;rsquo;. This is important mainly to manage URL&amp;rsquo;s as by default they have IP address.</description>
    </item>
    
    <item>
      <title>Install</title>
      <link>https://jaikk.github.io/pySparkPath/pyspark/install/</link>
      <pubDate>Wed, 27 Jan 2021 21:34:27 +1100</pubDate>
      
      <guid>https://jaikk.github.io/pySparkPath/pyspark/install/</guid>
      <description>Create a folder named &amp;ldquo;pySpark&amp;rdquo;. Create a virtual environment &amp;ldquo;venv&amp;rdquo; in &amp;ldquo;pySpark&amp;rdquo; folder
(base) C:\Users\Aadi\pyspark&amp;gt;.\venv\Scripts\activate.bat (venv) (base) C:\Users\Aadi\pyspark&amp;gt;pip install pyspark (base) C:\Users\Aadi\pyspark&amp;gt;.\venv\Scripts\activate.bat (venv) (base) C:\Users\Aadi\pyspark&amp;gt;pip --version pip 20.1.1 from c:\users\aadi\pyspark\venv\lib\site-packages\pip (python 3.8) (venv) (base) C:\Users\Aadi\pyspark&amp;gt;pip install pyspark Collecting pyspark Using cached pyspark-3.0.1.tar.gz (204.2 MB) Collecting py4j==0.10.9 Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB) Using legacy setup.py install for pyspark, since package &#39;wheel&#39; is not installed. Installing collected packages: py4j, pyspark Running setup.py install for pyspark .</description>
    </item>
    
  </channel>
</rss>

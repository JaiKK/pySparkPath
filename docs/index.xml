<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pySparkPath on pySpark Learning Path</title>
    <link>https://jaikk.github.io/pySparkPath/</link>
    <description>Recent content in pySparkPath on pySpark Learning Path</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Jan 2021 19:36:53 +1100</lastBuildDate><atom:link href="https://jaikk.github.io/pySparkPath/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Installation</title>
      <link>https://jaikk.github.io/pySparkPath/pyspark/installation/</link>
      <pubDate>Sun, 31 Jan 2021 23:33:26 +1100</pubDate>
      
      <guid>https://jaikk.github.io/pySparkPath/pyspark/installation/</guid>
      <description>Create Dockerfile This installation method will cover installation of Spark in local system (Linux) and docker container. I have attached Dockerfile with this tutorial and explained each line and its purpose.
I have used Ubuntu for this tutorial but you can choose any linux distribution. Windows I&amp;rsquo;ll cover is separate tutorial.
In this tutorial I have downloaded the spark installer (compressed file) for convenience. But I&amp;rsquo;ll also add commands to download it directly from server while installation.</description>
    </item>
    
    <item>
      <title>Running</title>
      <link>https://jaikk.github.io/pySparkPath/pyspark/running/</link>
      <pubDate>Sun, 14 Feb 2021 08:44:28 +1100</pubDate>
      
      <guid>https://jaikk.github.io/pySparkPath/pyspark/running/</guid>
      <description>Use below command to run Docker container $&amp;gt; docker run -dit -p4040:4040 -p8888:8888 jai/pyspark bash  After logging on container you can run below commands
 $&amp;gt; spark-shell $&amp;gt; spark-submit $&amp;gt; pyspark $&amp;gt; spark-sql $&amp;gt; sparkR $&amp;gt; spark-class Below is to test Spark installation and test check examples
$&amp;gt; /opt/spark/bin/run-example run-example SparkPi 10 Below is to interact with HiveServer2 client.
$&amp;gt; beeline To start Jupyter notebook server use below command</description>
    </item>
    
    <item>
      <title>Install</title>
      <link>https://jaikk.github.io/pySparkPath/pyspark/install/</link>
      <pubDate>Wed, 27 Jan 2021 21:34:27 +1100</pubDate>
      
      <guid>https://jaikk.github.io/pySparkPath/pyspark/install/</guid>
      <description>Create a folder named &amp;ldquo;pySpark&amp;rdquo;. Create a virtual environment &amp;ldquo;venv&amp;rdquo; in &amp;ldquo;pySpark&amp;rdquo; folder
(base) C:\Users\Aadi\pyspark&amp;gt;.\venv\Scripts\activate.bat (venv) (base) C:\Users\Aadi\pyspark&amp;gt;pip install pyspark (base) C:\Users\Aadi\pyspark&amp;gt;.\venv\Scripts\activate.bat (venv) (base) C:\Users\Aadi\pyspark&amp;gt;pip --version pip 20.1.1 from c:\users\aadi\pyspark\venv\lib\site-packages\pip (python 3.8) (venv) (base) C:\Users\Aadi\pyspark&amp;gt;pip install pyspark Collecting pyspark Using cached pyspark-3.0.1.tar.gz (204.2 MB) Collecting py4j==0.10.9 Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB) Using legacy setup.py install for pyspark, since package &#39;wheel&#39; is not installed. Installing collected packages: py4j, pyspark Running setup.py install for pyspark .</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>https://jaikk.github.io/pySparkPath/venv/setup/</link>
      <pubDate>Wed, 27 Jan 2021 19:32:42 +1100</pubDate>
      
      <guid>https://jaikk.github.io/pySparkPath/venv/setup/</guid>
      <description>Reference video: https://www.youtube.com/watch?v=APOPm01BVrk
Reference document: https://docs.python.org/3/library/venv.html
Command: python -m venv By convention its good to create virtual environment with name of &amp;ldquo;venv&amp;rdquo;. This we can add in gitignore file as well.
python -m venv venv First venv in above command is module name and second is the name of environment to be created.
(base) C:\Users\Aadi&amp;gt;python --version (base) C:\Users\Aadi&amp;gt;python -m venv --help (base) C:\Users\Aadi&amp;gt;mkdir pyspark (base) C:\Users\Aadi&amp;gt;cd pyspark (base) C:\Users\Aadi\pyspark&amp;gt;python -m venv venv (base) C:\Users\Aadi\pyspark&amp;gt;.</description>
    </item>
    
  </channel>
</rss>
